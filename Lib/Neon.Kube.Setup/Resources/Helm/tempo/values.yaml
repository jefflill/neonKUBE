global:
  image:
    # -- Overrides the Docker registry globally for all images
    registry: null
  # -- Overrides the priorityClassName for all pods
  priorityClassName: null
  # -- configures cluster domain ("cluster.local" by default)
  clusterDomain: "cluster.local"
  # -- configures DNS service name
  dnsService: "kube-dns"
  # -- configures DNS service namespace
  dnsNamespace: "kube-system"

# -- Overrides the chart's name
nameOverride: "tempo"

# -- Overrides the chart's computed fullname
fullnameOverride: "tempo"

serviceMesh:
  enabled: false

tempo:
  image:
    # -- The Docker registry
    registry: neon-registry.node.local
    # -- Docker image repository
    repository: grafana-tempo
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: 1.3.2
    pullPolicy: IfNotPresent
  readinessProbe:
    httpGet:
      path: /ready
      port: http
    initialDelaySeconds: 30
    timeoutSeconds: 1
  # -- Global labels for all tempo pods
  podLabels: {}
  # -- Common annotations for all pods
  podAnnotations: {}
  # --- SecurityContext holds pod-level security attributes and common container settings
  securityContext:
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
  #  capabilities:
  #    drop:
  #    - ALL
  #  readOnlyRootFilesystem: true
  #  runAsNonRoot: true
  #  runAsUser: 1000
  memBallastSizeMbs: 64

serviceAccount:
  # -- Specifies whether a ServiceAccount should be created
  create: true
  # -- The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name: null
  # -- Image pull secrets for the service account
  imagePullSecrets: []
  # -- Annotations for the service account
  annotations: {}

rbac:
  # -- Specifies whether RBAC manifests should be created
  create: false
  # -- Specifies whether a PodSecurityPolicy should be created
  pspEnabled: false

# Configuration for the ingester
ingester:
  # -- Number of replicas for the ingester
  replicas: 1
  image:
    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`
    registry: null
    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`
    tag: null
  # -- The name of the PriorityClass for ingester pods
  priorityClassName: null
  # -- Labels for ingester pods
  podLabels: {}
  # -- Annotations for ingester pods
  podAnnotations: {}
  # -- Annotations for ingester deployment
  annotations: 
    reloader.stakater.com/search: 'true'
  # -- Additional CLI args for the ingester
  extraArgs: []
  # -- Environment variables to add to the ingester pods
  extraEnv:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: minio
          key: accesskey
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: minio
          key: secretkey
    - name: GOGC
      value: "10"
  # -- Environment variables from secrets or configmaps to add to the ingester pods
  extraEnvFrom: []
  # -- Resource requests and limits for the ingester
  resources: {}
  securityContext:
    fsGroup: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,
  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring
  # all data and to successfully leave the member ring on shutdown.
  terminationGracePeriodSeconds: 300
  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: tempo
              app.kubernetes.io/component: ingester
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: tempo
                app.kubernetes.io/instance: tempo
                app.kubernetes.io/component: ingester
            topologyKey: failure-domain.beta.kubernetes.io/zone
  # -- Node selector for ingester pods
  nodeSelector:
    neonkube.io/monitor.traces-internal: 'true'
  # -- Tolerations for ingester pods
  tolerations: []
  # -- Extra volumes for ingester pods
  extraVolumeMounts: []
  # -- Extra volumes for ingester deployment
  extraVolumes: []
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: true
    # -- Size of persistent disk
    size: 5Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: neon-internal-tempo
  config:
    # -- Maximum size of a block before cutting it
    max_block_bytes: null
    # -- Maximum length of time before cutting a block
    max_block_duration: null
    # -- Duration to keep blocks in the ingester after they have been flushed
    complete_block_timeout: null

distributor:
  # -- Number of replicas for the distributor
  replicas: 1
  image:
    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`
    registry: null
    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`
    tag: null
  service:
    # -- Annotations for distributor service
    annotations: {}
    # -- Type of service for the distributor
    type: ClusterIP
    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
    loadBalancerIP: ""
    # -- If type is LoadBalancer limit incoming traffic from IPs.
    loadBalancerSourceRanges: []
  # -- The name of the PriorityClass for distributor pods
  priorityClassName: null
  # -- Labels for distributor pods
  podLabels: {}
  # -- Annotations for distributor pods
  podAnnotations: {}
  # -- Annotations for distributor deployment
  annotations:
    reloader.stakater.com/search: 'true'
  # -- Additional CLI args for the distributor
  extraArgs: []
  # -- Environment variables to add to the distributor pods
  extraEnv:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: minio
          key: accesskey
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: minio
          key: secretkey
    - name: GOGC
      value: "10"
  # -- Environment variables from secrets or configmaps to add to the distributor pods
  extraEnvFrom: []
  # -- Resource requests and limits for the distributor
  resources: {}
  # -- Grace period to allow the distributor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: tempo
              app.kubernetes.io/component: distributor
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: tempo
                app.kubernetes.io/instance: tempo
                app.kubernetes.io/component: distributor
            topologyKey: failure-domain.beta.kubernetes.io/zone
  # -- Node selector for distributor pods
  nodeSelector:
    neonkube.io/monitor.traces-internal: 'true'
  # -- Tolerations for distributor pods
  tolerations: []
  # -- Extra volumes for distributor pods
  extraVolumeMounts: []
  # -- Extra volumes for distributor deployment
  extraVolumes: []

compactor:
  image:
    # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`
    registry: null
    # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`
    repository: null
    # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`
    tag: null
  # -- The name of the PriorityClass for compactor pods
  priorityClassName: null
  # -- Labels for compactor pods
  podLabels: {}
  # -- Annotations for compactor pods
  podAnnotations: {}
  # -- Annotations for compactor deployment
  annotations:
    reloader.stakater.com/search: 'true'
  # -- Additional CLI args for the compactor
  extraArgs: []
  # -- Environment variables to add to the compactor pods
  extraEnv:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: minio
          key: accesskey
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: minio
          key: secretkey
    - name: GOGC
      value: "10"
  # -- Environment variables from secrets or configmaps to add to the compactor pods
  extraEnvFrom: []
  # -- Resource requests and limits for the compactor
  resources: {}
  # -- Grace period to allow the compactor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Node selector for compactor pods
  nodeSelector:
    neonkube.io/monitor.traces-internal: 'true'
  # -- Tolerations for compactor pods
  tolerations: []
  # -- Extra volumes for compactor pods
  extraVolumeMounts: []
  # -- Extra volumes for compactor deployment
  extraVolumes: []
  config:
    compaction:
      # -- Duration to keep blocks
      block_retention: 48h

# Configuration for the querier
querier:
  # -- Number of replicas for the querier
  replicas: 1
  image:
    # -- The Docker registry for the querier image. Overrides `tempo.image.registry`
    registry: null
    # -- Docker image repository for the querier image. Overrides `tempo.image.repository`
    repository: null
    # -- Docker image tag for the querier image. Overrides `tempo.image.tag`
    tag: null
  # -- The name of the PriorityClass for querier pods
  priorityClassName: null
  # -- Labels for querier pods
  podLabels: {}
  # -- Annotations for querier pods
  podAnnotations: {}
  # -- Annotations for querier deployment
  annotations: 
    reloader.stakater.com/search: 'true'
  # -- Additional CLI args for the querier
  extraArgs: []
  # -- Environment variables to add to the querier pods
  extraEnv:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: minio
          key: accesskey
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: minio
          key: secretkey
    - name: GOGC
      value: "10"
  # -- Environment variables from secrets or configmaps to add to the querier pods
  extraEnvFrom: []
  # -- Resource requests and limits for the querier
  resources: {}
  # -- Grace period to allow the querier to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: tempo
              app.kubernetes.io/component: querier
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: tempo
                app.kubernetes.io/instance: tempo
                app.kubernetes.io/component: querier
            topologyKey: failure-domain.beta.kubernetes.io/zone
  # -- Node selector for querier pods
  nodeSelector:
    neonkube.io/monitor.traces-internal: 'true'
  # -- Tolerations for querier pods
  tolerations: []
  # -- Extra volumes for querier pods
  extraVolumeMounts: []
  # -- Extra volumes for querier deployment
  extraVolumes: []
  config:
    frontend_worker:
      # -- grpc client configuration
      grpc_client_config: {}

# Configuration for the query-frontend
queryFrontend:
  query:
    # -- Required for grafana version <7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch
    enabled: true
    image:
      # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
      registry: null
      # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
      repository: grafana-tempo-query
      # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
      tag: null
    # -- Resource requests and limits for the query
    resources: {}
    # -- Additional CLI args for tempo-query pods
    extraArgs: []
    # -- Environment variables to add to the tempo-query pods
    extraEnv:
      - name: ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio
            key: accesskey
      - name: SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: secretkey
      - name: GOGC
        value: "10"
    # -- Environment variables from secrets or configmaps to add to the tempo-query pods
    extraEnvFrom: []
    # -- Extra volumes for tempo-query pods
    extraVolumeMounts: []
    # -- Extra volumes for tempo-query deployment
    extraVolumes: []
    config: |
      backend: 127.0.0.1:3100
  # -- Number of replicas for the query-frontend
  replicas: 1
  image:
    # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
    registry: null
    # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
    repository: null
    # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
    tag: null
  service:
    # -- Annotations for queryFrontend service
    annotations: {}
    # -- Type of service for the queryFrontend
    type: ClusterIP
  serviceDiscovery:
    # -- Annotations for queryFrontendDiscovery service
    annotations: {}
  # -- The name of the PriorityClass for query-frontend pods
  priorityClassName: null
  # -- Labels for queryFrontend pods
  podLabels: {}
  # -- Annotations for query-frontend pods
  podAnnotations: {}
  # -- Annotations for query-frontend deployment
  annotations: 
    reloader.stakater.com/search: 'true'
  # -- Additional CLI args for the query-frontend
  extraArgs: []
  # -- Environment variables to add to the query-frontend pods
  extraEnv:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: minio
          key: accesskey
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: minio
          key: secretkey
    - name: GOGC
      value: "10"
  # -- Environment variables from secrets or configmaps to add to the query-frontend pods
  extraEnvFrom: []
  # -- Resource requests and limits for the query-frontend
  resources: {}
  # -- Grace period to allow the query-frontend to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: tempo
              app.kubernetes.io/component: query-frontend
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: tempo
                app.kubernetes.io/instance: tempo
                app.kubernetes.io/component: query-frontend
            topologyKey: failure-domain.beta.kubernetes.io/zone
  # -- Node selector for query-frontend pods
  nodeSelector:
    neonkube.io/monitor.traces-internal: 'true'
  # -- Tolerations for query-frontend pods
  tolerations: []
  # -- Extra volumes for query-frontend pods
  extraVolumeMounts: []
  # -- Extra volumes for query-frontend deployment
  extraVolumes: []

search:
  # -- Enable Tempo search
  enabled: true

traces:
  jaeger:
    # -- Enable Tempo to ingest Jaeger GRPC traces
    grpc: true
    # -- Enable Tempo to ingest Jaeger Thrift Binary traces
    thriftBinary: true
    # -- Enable Tempo to ingest Jaeger Thrift Compact traces
    thriftCompact: true
    # -- Enable Tempo to ingest Jaeger Thrift HTTP traces
    thriftHttp: true
  # -- Enable Tempo to ingest Zipkin traces
  zipkin: true
  otlp:
    # -- Enable Tempo to ingest Open Telemetry HTTP traces
    http: true
    # -- Enable Tempo to ingest Open Telemetry GRPC traces
    grpc: true
  # -- Enable Tempo to ingest Open Census traces
  opencensus: true
  # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver
  kafka: {}

config: |
  multitenancy_enabled: true
  search_enabled: {{ .Values.search.enabled }}
  compactor:
    compaction:
      block_retention: {{ .Values.compactor.config.compaction.block_retention }}
    ring:
      kvstore:
        store: memberlist
  distributor:
    ring:
      kvstore:
        store: memberlist
    receivers:
      {{- if  or (.Values.traces.jaeger.thriftCompact) (.Values.traces.jaeger.thriftBinary) (.Values.traces.jaeger.thriftHttp) (.Values.traces.jaeger.grpc) }}
      jaeger:
        protocols:
          {{- if .Values.traces.jaeger.thriftCompact }}
          thrift_compact:
            endpoint: 0.0.0.0:6831
          {{- end }}
          {{- if .Values.traces.jaeger.thriftBinary }}
          thrift_binary:
            endpoint: 0.0.0.0:6832
          {{- end }}
          {{- if .Values.traces.jaeger.thriftHttp }}
          thrift_http:
            endpoint: 0.0.0.0:14268
          {{- end }}
          {{- if .Values.traces.jaeger.grpc }}
          grpc:
            endpoint: 0.0.0.0:14250
          {{- end }}
      {{- end }}
      {{- if .Values.traces.zipkin}}
      zipkin:
        endpoint: 0.0.0.0:9411
      {{- end }}
      {{- if or (.Values.traces.otlp.http) (.Values.traces.otlp.grpc) }}
      otlp:
        protocols:
          {{- if .Values.traces.otlp.http }}
          http:
            endpoint: 0.0.0.0:55681
          {{- end }}
          {{- if .Values.traces.otlp.grpc }}
          grpc:
            endpoint: 0.0.0.0:4317
          {{- end }}
      {{- end }}
      {{- if .Values.traces.opencensus }}
      opencensus:
        endpoint: 0.0.0.0:55678
      {{- end }}
      {{- if .Values.traces.kafka }}
      kafka:
        {{- toYaml .Values.traces.kafka | nindent 6 }}
      {{- end }}
  querier:
    frontend_worker:
      frontend_address: {{ include "tempo.queryFrontendFullname" . }}-discovery:9095
      {{- if .Values.querier.config.frontend_worker.grpc_client_config }}
      grpc_client_config:
        {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}
      {{- end }}
  ingester:
    lifecycler:
      ring:
        replication_factor: 1
        kvstore:
          store: memberlist
      tokens_file_path: /var/tempo/tokens.json
    {{- if .Values.ingester.config.maxBlockBytes }}
    max_block_bytes: {{ .Values.ingester.config.maxBlockBytes }}
    {{- end }}
    {{- if .Values.ingester.config.maxBlockDuration }}
    max_block_duration: {{ .Values.ingester.config.maxBlockDuration }}
    {{- end }}
    {{- if .Values.ingester.config.completeBlockTimeout }}
    complete_block_timeout: {{ .Values.ingester.config.completeBlockTimeout }}
    {{- end }}
  memberlist:
    abort_if_cluster_join_fails: false
    join_members:
      - {{ include "tempo.fullname" . }}-gossip-ring
  overrides:
    {{- toYaml .Values.global_overrides | nindent 2 }}
  server:
    http_listen_port: {{ .Values.server.httpListenPort }}
    log_level: {{ .Values.server.logLevel }}
    log_format: {{ .Values.server.logFormat }}
    grpc_server_max_recv_msg_size: 4194304
    grpc_server_max_send_msg_size: 4194304
  storage:
    trace:
      backend: {{.Values.storage.trace.backend}}
      {{- if eq .Values.storage.trace.backend "gcs"}}
      gcs:
        {{- toYaml .Values.storage.trace.gcs | nindent 6}}
      {{- end}}
      {{- if eq .Values.storage.trace.backend "s3"}}
      s3:
        {{- toYaml .Values.storage.trace.s3 | nindent 6}}
      {{- end}}
      {{- if eq .Values.storage.trace.backend "azure"}}
      azure:
        {{- toYaml .Values.storage.trace.azure | nindent 6}}
      {{- end}}
      blocklist_poll: 5m
      local:
        path: /var/tempo/traces
      wal:
        path: /var/tempo/wal
      cache: memcached
      memcached:
        consistent_hash: true
        addresses: dns+neon-memcached.neon-system.svc:11211
        timeout: 500ms

# Set Tempo server configuration
# Refers to https://grafana.com/docs/tempo/latest/configuration/#server
server:
  # --  HTTP server listen host
  httpListenPort: 3100
  # -- Log level. Can be set to trace, debug, info (default), warn error, fatal, panic
  logLevel: info
  # -- Log format. Can be set to logfmt (default) or json.
  logFormat: json
# To configure a different storage backend instead of local storage:
# storage:
#   trace:
#     backend: azure
#     azure:
#       container-name:
#       storage-account-name:
#       storage-account-key:
# -- the supported storage backends are gcs, s3 and azure
# -- as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
storage:
  trace:
    backend: local
    s3:
      endpoint: minio.neon-system.svc:80
      bucket: tempo
      access_key: ${ACCESS_KEY_ID}
      secret_key: ${SECRET_ACCESS_KEY}
      insecure: true

# Global overrides
global_overrides:
  per_tenant_override_config: /conf/overrides.yaml

# Per tenants overrides
overrides: |
  overrides: {}

# memcached is for all of the Tempo pieces to coordinate with each other.
# you can use your self memcacherd by set enable: false and host + service
memcached:
  # -- Specified whether the memcached cachce should be enabled
  enabled: true

# ServiceMonitor configuration
serviceMonitor:
  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
  enabled: false
  # -- Alternative namespace for ServiceMonitor resources
  namespace: null
  # -- Namespace selector for ServiceMonitor resources
  namespaceSelector: {}
  # -- ServiceMonitor annotations
  annotations: {}
  # -- Additional ServiceMonitor labels
  labels: {}
  # -- ServiceMonitor scrape interval
  interval: null
  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
  scrapeTimeout: null
  # -- ServiceMonitor relabel configs to apply to samples before scraping
  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
  relabelings: []
  # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
  metricRelabelings: []
  # -- ServiceMonitor will use http by default, but you can pick https as well
  scheme: http
  # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
  tlsConfig: null

# Rules for the Prometheus Operator
prometheusRule:
  # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
  enabled: false
  # -- Alternative namespace for the PrometheusRule resource
  namespace: null
  # -- PrometheusRule annotations
  annotations: {}
  # -- Additional PrometheusRule labels
  labels: {}
  # -- Contents of Prometheus rules file
  groups: []
  # - name: loki-rules
  #   rules:
  #     - record: job:loki_request_duration_seconds_bucket:sum_rate
  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
  #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate
  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
  #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
  #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)

# Configuration for istio gateway
istio:
  service:
    # -- Port of the nginx service
    port: 80
    # -- Type of the nginx service
    type: ClusterIP
    # -- ClusterIP of the nginx service
    clusterIP: null
    # -- Node port if service type is NodePort
    nodePort: null
    # -- Annotations for the nginx service
    annotations: {}
    # -- Labels for nginx service
    labels: {}
